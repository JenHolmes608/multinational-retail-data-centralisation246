{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import jpype\n",
    "import yaml\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from sqlalchemy import inspect\n",
    "from sqlalchemy import text\n",
    "from pandasgui import show\n",
    "import numpy as np \n",
    "import tabula\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseConnector:\n",
    "    def __init__(self, yaml_file_path = 'db_creds.yaml'):\n",
    "        self.engine = self.init_db_engine()\n",
    "\n",
    "    def read_db_creds(self):\n",
    "        with open('db_creds.yaml', 'r') as file:\n",
    "            db_creds = yaml.safe_load(file)\n",
    "            return db_creds\n",
    "\n",
    "    def init_db_engine(self):\n",
    "        engine = create_engine(f\"postgresql://{self.read_db_creds()['RDS_USER']}:{self.read_db_creds()['RDS_PASSWORD']}@{self.read_db_creds()['RDS_HOST']}:{self.read_db_creds()['RDS_PORT']}/{self.read_db_creds()['RDS_DATABASE']}\")\n",
    "        engine.execution_options(isolation_level = 'AUTOCOMMIT').connect()\n",
    "        return engine\n",
    "        \n",
    "    def list_db_tables(self):\n",
    "        inspector = inspect(self.engine) \n",
    "        db_tables = inspector.get_table_names()\n",
    "        return db_tables\n",
    "    \n",
    "    def upload_to_db(self, df, table_name):\n",
    "        df.to_sql(table_name, con=self.engine, if_exists='replace', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExtractor:\n",
    "    def __init__(self, engine):\n",
    "        self.engine = engine\n",
    "        self.header_dictionary = {'x-api-key': 'yFBQbwXe9J3sd6zWVAMrK6lcxxr0q1lr2PT6DDMX'}\n",
    "        self.base_url = 'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/'\n",
    "        \n",
    "    def reads_rds_table(self, table_name):\n",
    "        data = pd.read_sql_table(table_name, self.engine)\n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "        \n",
    "    def retrieve_pdf_data(self, pdf_url):\n",
    "        df_list = tabula.read_pdf(pdf_url, pages='all')\n",
    "        extracted_data = pd.concat(df_list, ignore_index=True)\n",
    "        return extracted_data\n",
    "       \n",
    "    def list_number_of_stores(self, number_of_stores_endpoint):\n",
    "        response = requests.get(number_of_stores_endpoint, headers=self.header_dictionary)\n",
    "        print(response.json())\n",
    "        number_of_stores = response.json()['number_stores']\n",
    "        return number_of_stores\n",
    "    \n",
    "    def retrieve_stores_data(self, retrieve_store_endpoint, number_of_stores):\n",
    "        store_data_list = []\n",
    "        for store_number in range(0, number_of_stores):\n",
    "            endpoint_url = f\"{self.base_url}{retrieve_store_endpoint}/{store_number}\"\n",
    "            response = requests.get(endpoint_url, headers=self.header_dictionary)\n",
    "            store_data_list.append(response.json())\n",
    "        \n",
    "        store_df = pd.DataFrame(store_data_list)\n",
    "        return store_df\n",
    "    \n",
    "    def extract_from_s3(self, s3_address):\n",
    "        s3 = boto3.client('s3')\n",
    "        bucket, key = s3_address.split('//')[1].split('/', 1)\n",
    "        s3.download_file(bucket, key, 'products.csv')\n",
    "        df = pd.read_csv('products.csv')\n",
    "        return df\n",
    "    \n",
    "    def extract_date_events_data(self, date_events_url):\n",
    "        s3 = boto3.client('s3')\n",
    "        bucket, key = date_events_url.split('//')[1].split('/', 1)\n",
    "        s3.download_file(bucket, key, 'date_details.json')\n",
    "        df = pd.read_json('date_details.json')\n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "JVMNotFoundException",
     "evalue": "No JVM shared library file (jvm.dll) found. Try setting up the JAVA_HOME environment variable properly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJVMNotFoundException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m orders_df \u001b[38;5;241m=\u001b[39m Display_Data\u001b[38;5;241m.\u001b[39mreads_rds_table(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morders_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m pdf_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://data-handling-public.s3.eu-west-1.amazonaws.com/card_details.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 12\u001b[0m card_details_df \u001b[38;5;241m=\u001b[39m \u001b[43mDisplay_Data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_pdf_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m number_of_stores_endpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     16\u001b[0m retrieve_store_endpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstore_details\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m, in \u001b[0;36mDataExtractor.retrieve_pdf_data\u001b[1;34m(self, pdf_url)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_pdf_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, pdf_url):\n\u001b[1;32m---> 13\u001b[0m     df_list \u001b[38;5;241m=\u001b[39m \u001b[43mtabula\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     extracted_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(df_list, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m extracted_data\n",
      "File \u001b[1;32mc:\\Users\\holme\\miniconda3\\envs\\mrdc\\lib\\site-packages\\tabula\\io.py:395\u001b[0m, in \u001b[0;36mread_pdf\u001b[1;34m(input_path, output_format, encoding, java_options, pandas_options, multiple_tables, user_agent, use_raw_url, pages, guess, area, relative_area, lattice, stream, password, silent, columns, relative_columns, format, batch, output_path, force_subprocess, options)\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is empty. Check the file, or download it manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 395\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtabula_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjava_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_subprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_subprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m temporary:\n",
      "File \u001b[1;32mc:\\Users\\holme\\miniconda3\\envs\\mrdc\\lib\\site-packages\\tabula\\io.py:74\u001b[0m, in \u001b[0;36m_run\u001b[1;34m(options, java_options, path, encoding, force_subprocess)\u001b[0m\n\u001b[0;32m     69\u001b[0m     _tabula_vm \u001b[38;5;241m=\u001b[39m SubprocessTabula(\n\u001b[0;32m     70\u001b[0m         java_options\u001b[38;5;241m=\u001b[39mjava_options, silent\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39msilent, encoding\u001b[38;5;241m=\u001b[39mencoding\n\u001b[0;32m     71\u001b[0m     )\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _tabula_vm:\n\u001b[1;32m---> 74\u001b[0m     _tabula_vm \u001b[38;5;241m=\u001b[39m \u001b[43mTabulaVm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjava_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjava_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _tabula_vm \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _tabula_vm\u001b[38;5;241m.\u001b[39mtabula:\n\u001b[0;32m     76\u001b[0m         _tabula_vm \u001b[38;5;241m=\u001b[39m SubprocessTabula(\n\u001b[0;32m     77\u001b[0m             java_options\u001b[38;5;241m=\u001b[39mjava_options, silent\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39msilent, encoding\u001b[38;5;241m=\u001b[39mencoding\n\u001b[0;32m     78\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\holme\\miniconda3\\envs\\mrdc\\lib\\site-packages\\tabula\\backend.py:45\u001b[0m, in \u001b[0;36mTabulaVm.__init__\u001b[1;34m(self, java_options, silent)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m silent:\n\u001b[0;32m     37\u001b[0m         java_options\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m     38\u001b[0m             (\n\u001b[0;32m     39\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-Dorg.slf4j.simpleLogger.defaultLogLevel=off\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m             )\n\u001b[0;32m     43\u001b[0m         )\n\u001b[1;32m---> 45\u001b[0m     \u001b[43mjpype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartJVM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjava_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvertStrings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlang\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtechnology\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtabula\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtabula\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\holme\\miniconda3\\envs\\mrdc\\lib\\site-packages\\jpype\\_core.py:219\u001b[0m, in \u001b[0;36mstartJVM\u001b[1;34m(jvmpath, classpath, ignoreUnrecognized, convertStrings, interrupt, *jvmargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m         jvmargs \u001b[38;5;241m=\u001b[39m jvmargs[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jvmpath:\n\u001b[1;32m--> 219\u001b[0m     jvmpath \u001b[38;5;241m=\u001b[39m \u001b[43mgetDefaultJVMPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# Allow the path to be a PathLike.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     jvmpath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(jvmpath)\n",
      "File \u001b[1;32mc:\\Users\\holme\\miniconda3\\envs\\mrdc\\lib\\site-packages\\jpype\\_jvmfinder.py:74\u001b[0m, in \u001b[0;36mgetDefaultJVMPath\u001b[1;34m()\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     finder \u001b[38;5;241m=\u001b[39m LinuxJVMFinder()\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfinder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_jvm_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\holme\\miniconda3\\envs\\mrdc\\lib\\site-packages\\jpype\\_jvmfinder.py:212\u001b[0m, in \u001b[0;36mJVMFinder.get_jvm_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jvm_notsupport_ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m jvm_notsupport_ext\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m JVMNotFoundException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo JVM shared library file (\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    213\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound. Try setting up the JAVA_HOME \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    214\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment variable properly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    215\u001b[0m                            \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libfile))\n",
      "\u001b[1;31mJVMNotFoundException\u001b[0m: No JVM shared library file (jvm.dll) found. Try setting up the JAVA_HOME environment variable properly."
     ]
    }
   ],
   "source": [
    "RDS_CONNECTOR = DatabaseConnector()\n",
    "\n",
    "RDS_CONNECTOR.init_db_engine()\n",
    "       \n",
    "Display_Data = DataExtractor(RDS_CONNECTOR.engine)\n",
    "\n",
    "store_details_df = Display_Data.reads_rds_table(\"legacy_store_details\")\n",
    "user_data_df = Display_Data.reads_rds_table(\"legacy_users\")\n",
    "orders_df = Display_Data.reads_rds_table(\"orders_table\")\n",
    "\n",
    "pdf_url = 'https://data-handling-public.s3.eu-west-1.amazonaws.com/card_details.pdf'\n",
    "card_details_df = Display_Data.retrieve_pdf_data(pdf_url)\n",
    "\n",
    "\n",
    "number_of_stores_endpoint = 'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores'\n",
    "retrieve_store_endpoint = 'store_details'\n",
    "\n",
    "number_of_stores = Display_Data.list_number_of_stores(number_of_stores_endpoint)\n",
    "stores_df = Display_Data.retrieve_stores_data(retrieve_store_endpoint, number_of_stores)\n",
    "\n",
    "s3_address = 's3://data-handling-public/products.csv'\n",
    "product_df = Display_Data.extract_from_s3(s3_address)\n",
    "\n",
    "date_events_url = 'https://data-handling-public.s3.eu-west-1.amazonaws.com/date_details.json'\n",
    "date_events_df = Display_Data.extract_date_events_data(date_events_url)\n",
    "\n",
    "print(date_events_df.to_markdown())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaning:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def clean_user_data(self):\n",
    "        self.clean_legacy_users()\n",
    "        self.clean_orders_table()\n",
    "        self.clean_card_data()\n",
    "        self.clean_store_data()\n",
    "        self.convert_product_weights()\n",
    "        self.clean_products_data()\n",
    "        self.clean_date_events_data()\n",
    "        \n",
    "\n",
    "    def clean_legacy_users(self):\n",
    "        self.df = self.df.replace('NULL', np.nan)\n",
    "        self.df = self.df.replace('N/A', np.nan)\n",
    "        self.df.drop(self.df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "        self.df['date_of_birth'] = pd.to_datetime(self.df['date_of_birth'], errors='coerce', utc=False, format='mixed').dt.date\n",
    "        self.df['join_date'] = pd.to_datetime(self.df['join_date'], errors='coerce', utc=False, format='mixed').dt.date\n",
    "\n",
    "        self.df.dropna(axis=0, how='all', subset=self.df.columns[1:], inplace=True)\n",
    "        self.df = self.df.dropna(axis=1, how='all')\n",
    "        self.df = self.df.replace('NaT', np.nan)\n",
    "        self.df = self.df.dropna(subset=['date_of_birth'])\n",
    "\n",
    "        self.df.drop_duplicates(inplace=True)\n",
    "\n",
    "        phone_patterns = [r'^\\+\\d{1,3}-\\d{3}-\\d{3}-\\d{4}$', r'^\\d{3}-\\d{3}-\\d{4}$', r'^\\+49-\\d{3}-\\d{6,}$',\n",
    "                          r'^\\+44\\s?\\d{1,5}\\s?\\d{4}\\s?\\d{4}$', r'^\\+?[0-9()-]{7,}$']\n",
    "        valid_phone_numbers = self.df['phone_number'].str.match('|'.join(phone_patterns))\n",
    "        self.df.loc[~valid_phone_numbers, 'phone_number'] = np.nan\n",
    "\n",
    "        valid_email_addresses = self.df['email_address'].str.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$')\n",
    "        self.df.loc[~valid_email_addresses, 'email_address'] = np.nan\n",
    "\n",
    "        return self.df\n",
    "    \n",
    "    def clean_orders_data(self):\n",
    "\n",
    "        self.df.drop(columns=['first_name', 'last_name', '1'], inplace=True)\n",
    "\n",
    "        self.df = self.df.replace('NULL', np.nan)\n",
    "        self.df = self.df.replace('N/A', np.nan)\n",
    "        self.df.drop(self.df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "        self.df.dropna(axis=0, how='all', subset=self.df.columns[1:], inplace=True)\n",
    "        self.df = self.df.dropna(axis=1, how='all')\n",
    "\n",
    "        valid_card_numbers = self.df['card_number'].astype(str).apply(len).between(11, 19)\n",
    "        self.df['card_number'] = np.where(valid_card_numbers, self.df['card_number'].astype(str), np.nan)\n",
    "\n",
    "        return self.df\n",
    "    \n",
    "    def clean_card_data(self):\n",
    "        self.df = self.df.replace('NULL', np.nan)\n",
    "        self.df = self.df.replace('N/A', np.nan)\n",
    "\n",
    "        self.df['date_payment_confirmed'] = pd.to_datetime(self.df['date_payment_confirmed'], errors='coerce', utc=False, format='mixed').dt.date\n",
    "\n",
    "        self.df.dropna(axis=0, how='all', subset=self.df.columns[1:], inplace=True)\n",
    "        self.df = self.df.dropna(axis=1, how='all')\n",
    "        self.df = self.df.replace('NaT', np.nan)\n",
    "        self.df = self.df.dropna(subset=['date_payment_confirmed'])\n",
    "\n",
    "        self.df.drop_duplicates(inplace=True)\n",
    "\n",
    "        return self.df\n",
    "    \n",
    "    def clean_store_data(self):\n",
    "        self.df['lat'] = np.nan\n",
    "        self.df = self.df.replace('NULL', np.nan)\n",
    "        self.df = self.df.replace('N/A', np.nan)\n",
    "        self.df.drop(self.df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "        self.df['opening_date'] = pd.to_datetime(self.df['opening_date'], errors='coerce', utc=False, format='mixed').dt.date\n",
    "\n",
    "        self.df.dropna(axis=0, how='all', subset=self.df.columns[1:], inplace=True)\n",
    "        self.df = self.df.dropna(axis=1, how='all')\n",
    "        self.df = self.df.replace('NaT', np.nan)\n",
    "        self.df = self.df.dropna(subset=['opening_date'])\n",
    "\n",
    "        self.df['continent'] = self.df['continent'].replace('eeEurope', 'Europe')\n",
    "        self.df['continent'] = self.df['continent'].replace('eeAmerica', 'America')\n",
    "        self.df['staff_numbers'] = self.df['staff_numbers'].replace('e30', 30)\n",
    "\n",
    "        return self.df\n",
    "    \n",
    "   \n",
    "    def convert_product_weights(self):\n",
    "        def clean_and_convert(weight_str):\n",
    "            weight_str = weight_str.replace('g', '').replace('ml', '').replace(' ', '')\n",
    "            parts = weight_str.split('x')\n",
    "            total_weight = 0\n",
    "\n",
    "            for part in parts:\n",
    "                part = part.strip()\n",
    "                if part.endswith('g'):\n",
    "                    weight = float(part[:-1]) / 1000 \n",
    "                elif part.endswith('ml'):\n",
    "                    weight = float(part[:-2]) / 1000 \n",
    "                else:\n",
    "                    weight = float(part) \n",
    "                total_weight += weight\n",
    "\n",
    "            return total_weight\n",
    "\n",
    "        self.df['weight_in_kg'] = self.df['weight'].apply(clean_and_convert)\n",
    "\n",
    "        return self.df\n",
    "    \n",
    "    def clean_products_data(self):\n",
    "        self.df = self.df.replace('NULL', np.nan)\n",
    "        self.df = self.df.replace('N/A', np.nan)\n",
    "        self.df.drop(self.df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "        self.df['date_added'] = pd.to_datetime(self.df['date_added'], errors='coerce', utc=False, format='mixed').dt.date\n",
    "\n",
    "        self.df.dropna(axis=0, how='all', subset=self.df.columns[1:], inplace=True)\n",
    "        self.df = self.df.dropna(axis=1, how='all')\n",
    "        self.df = self.df.replace('NaT', np.nan)\n",
    "        self.df = self.df.dropna(subset=['date_added'])\n",
    "\n",
    "        return self.df\n",
    "    \n",
    "    def clean_date_events_data(self):\n",
    "        \n",
    "\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_user_data = DataCleaning(user_data_df)\n",
    "cleaned_user_data = clean_user_data.clean_legacy_users()\n",
    "RDS_CONNECTOR.upload_to_db(cleaned_user_data, 'dim_users')\n",
    "\n",
    "clean_orders_data = DataCleaning(orders_df)\n",
    "cleaned_orders_data = clean_orders_data.clean_orders_data()\n",
    "RDS_CONNECTOR.upload_to_db(cleaned_orders_data, 'orders_table')\n",
    "\n",
    "clean_card_details_data = DataCleaning(card_details_df)\n",
    "cleaned_card_details_data = clean_card_details_data.clean_card_data()\n",
    "RDS_CONNECTOR.upload_to_db(cleaned_card_details_data, 'dim_card_details')\n",
    "\n",
    "clean_stores_data = DataCleaning(stores_df)\n",
    "cleaned_stores_data = clean_stores_data.clean_store_data()\n",
    "RDS_CONNECTOR.upload_to_db(cleaned_stores_data, 'dim_store_details')\n",
    "\n",
    "clean_product_data = DataCleaning(product_df)\n",
    "cleaned_product_data = clean_product_data.convert_product_weights()\n",
    "cleaned_product_data = clean_product_data.clean_products_data()\n",
    "RDS_CONNECTOR.upload_to_db(cleaned_product_data, 'dim_products')\n",
    "\n",
    "clean_date_events_data = DataCleaning(date_events_df)\n",
    "cleaned_date_events_data = clean_date_events_data.clean_data_events()\n",
    "RDS_CONNECTOR.upload_to_db(cleaned_date_events_data, 'dim_dates_times')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrdc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
